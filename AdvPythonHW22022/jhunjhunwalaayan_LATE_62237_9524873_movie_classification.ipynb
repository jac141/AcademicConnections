{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "movie_classification",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "q9lDF9fhhk_l"
      },
      "outputs": [],
      "source": [
        "#We are going to import all necessary libraries\n",
        "import numpy as np\n",
        "from tensorflow import keras#tensorflow instead of pytorch\n",
        "from tensorflow.keras import layers\n",
        "max_features = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Add 2 bidirectional LSTMs\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)#Squish function\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XknB9nYFllpi",
        "outputId": "3a90a754-d101-4678-b3d1-6364284d59e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, None, 128)        98816     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 128)              98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,757,761\n",
            "Trainable params: 2,757,761\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n",
        "    num_words=max_features\n",
        ")\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHxR2p96-6HQ",
        "outputId": "523d865d-1447-44a7-fb93-2f2763df5872"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])#This takes about 7 minutes to run and the accuracy I got was 89%\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))\n",
        "#This example led me to dive deeper into LSTM(Long short term memory) models. They are a form of recurrent neural networks which re input previous information along with new information to more accurate learning\n",
        "#This type of neural network is perfect for imdb movie recognition as it can recognize sentiment as it goes through more and more movie reviews. This was something we did not do in class. I also did a litle \n",
        "#more research and I found out that LSTM networks are actually very good for NLPs and I plan on trying another example in my free time which links the two."
      ],
      "metadata": {
        "id": "DseCWoYP-9RT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}